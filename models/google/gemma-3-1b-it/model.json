{
  "canonical_model_id": null,
  "fine_tuned_from_model_id": null,
  "name": "Gemma 3 1B",
  "description": "The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",
  "release_date": "2025-03-12",
  "input_context_size": 32000,
  "output_context_size": 8192,
  "license": "gemma",
  "multimodal": false,
  "web_hydrated": false,
  "knowledge_cutoff": null,
  "api_ref_link": "https://huggingface.co/google/gemma-3-1b-it",
  "playground_link": "https://huggingface.co/chat/models/google/gemma-3-1b-it",
  "paper_link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
  "scorecard_blog_link": "https://huggingface.co/blog/gemma3",
  "repo_link": null,
  "weights_link": "https://huggingface.co/google/gemma-3-1b-it",
  "param_count": 1000000000,
  "training_tokens": 2000000000000,
  "qualitative_metrics": [
    {
      "dataset_name": "GPQA",
      "score": 0.192,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation diamond",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "SimpleQA",
      "score": 0.022,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "FACTS Grounding",
      "score": 0.364,
      "is_self_reported": true,
      "analysis_method": "- evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "BIG-Bench Hard",
      "score": 0.391,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "BIG-Bench Extra Hard",
      "score": 0.072,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "IFEval",
      "score": 0.802,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "MMLU-Pro",
      "score": 0.147,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "LiveCodeBench",
      "score": 0.019,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "Bird-SQL (dev)",
      "score": 0.064,
      "is_self_reported": true,
      "analysis_method": "- evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "MATH",
      "score": 0.48,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "HiddenMath",
      "score": 0.158,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "MBPP",
      "score": 0.352,
      "is_self_reported": true,
      "analysis_method": "3-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "HumanEval",
      "score": 0.415,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "Natural2Code",
      "score": 0.56,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "GSM8K",
      "score": 0.628,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "Global-MMLU-Lite",
      "score": 0.342,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "ECLeKTic",
      "score": 0.014,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    },
    {
      "dataset_name": "WMT24++",
      "score": 0.359,
      "is_self_reported": true,
      "analysis_method": "0-shot evaluation",
      "date_recorded": "2025-03-12",
      "source_link": "https://ai.google.dev/gemma/docs/core/model_card_3"
    }
  ]
}
